{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CM++ Pipeline Customizable modular pipeline for testing an improved version of CM for generating well-connected clusters. Image below from arXiv preprint: Park et. al. (2023). (Original Source Code) Quick Start Guide Features Installation Quick Start Guide Input and Usage Example Commands CM++ Usage CM Pipeline Usage JSON Format Tutorial Contributing Contibuting Developer Setup Archive View Old Release Notes Citations @misc{cm_pipe2023, author = {Vikram Ramavarapu and Vidya Kamath and Minhyuk Park and Fabio Ayres and George Chacko}, title = {Connectivity Modifier Pipeline}, howpublished = {\\url{https://github.com/illinois-or-research-analytics/cm_pipeline}}, year={2023}, doi={10.5281/zenodo.10076514} } @misc{park2023wellconnected, title={Well-Connected Communities in Real-World and Synthetic Networks}, author={Minhyuk Park and Yasamin Tabatabaee and Vikram Ramavarapu and Baqiao Liu and Vidya Kamath Pailodi and Rajiv Ramachandran and Dmitriy Korobskiy and Fabio Ayres and George Chacko and Tandy Warnow}, year={2023}, eprint={2303.02813}, archivePrefix={arXiv}, primaryClass={cs.SI} }","title":"CM++ Pipeline"},{"location":"#cm-pipeline","text":"Customizable modular pipeline for testing an improved version of CM for generating well-connected clusters. Image below from arXiv preprint: Park et. al. (2023). (Original Source Code)","title":"CM++ Pipeline"},{"location":"#quick-start-guide","text":"Features Installation Quick Start Guide","title":"Quick Start Guide"},{"location":"#input-and-usage","text":"Example Commands CM++ Usage CM Pipeline Usage JSON Format Tutorial","title":"Input and Usage"},{"location":"#contributing","text":"Contibuting Developer Setup","title":"Contributing"},{"location":"#archive","text":"View Old Release Notes","title":"Archive"},{"location":"#citations","text":"@misc{cm_pipe2023, author = {Vikram Ramavarapu and Vidya Kamath and Minhyuk Park and Fabio Ayres and George Chacko}, title = {Connectivity Modifier Pipeline}, howpublished = {\\url{https://github.com/illinois-or-research-analytics/cm_pipeline}}, year={2023}, doi={10.5281/zenodo.10076514} } @misc{park2023wellconnected, title={Well-Connected Communities in Real-World and Synthetic Networks}, author={Minhyuk Park and Yasamin Tabatabaee and Vikram Ramavarapu and Baqiao Liu and Vidya Kamath Pailodi and Rajiv Ramachandran and Dmitriy Korobskiy and Fabio Ayres and George Chacko and Tandy Warnow}, year={2023}, eprint={2303.02813}, archivePrefix={arXiv}, primaryClass={cs.SI} }","title":"Citations"},{"location":"cmpp/","text":"CM++ Usage Basic Usage python -m hm01.cm -i {input network} -e {existing clustering (optional)} -c {clustering algorithm} -t {connectivity threshold} -n {number of processors (optional)} -o {output file} -i Input Network : The input .tsv edgelist filename. -e Existing Clustering : The existing clustering filename. This is a .tsv with each line being 'node_id cluster_id' format. If this isn't provided, CM++ will run an initial clustering on its own. -c Clustering Algorithm : The clustering paradigm to be used by CM++. Can choose from leiden , leiden_mod , ikc , and external . -t Connectivity Threshold : The connectivity threshold that every cluster must have at least to be considered 'well-connected'. This threshold can be a static integer e.g. 2 or a linear combination of: log10 : $log10(n)$ where $n$ is the number of nodes in the cluster k : the k-core value of the cluster mcd : the minimum degree value e.g. 1log10 , 1log10+1mcd+2k . -n Number of Processors : The number of cores to run CM++ in parallel. This defaults to 4. -o Output File : The output clustering file path. This is a 'node_id cluster_id' .tsv. For example, you can run: python -m hm01.cm -i network.tsv -e clustering.tsv -c leiden -g 0.01 -t 1log10 -n 32 -o output.tsv External Clusterers If you want to use an external clustering algorithm, use the following command format: python -m hm01.cm -i {input network} -e {existing clustering (optional)} -c external -cargs {clustering arguments json} -cfile {clustering algorithm code} -t {connectivity threshold} -n {number of processors (optional)} -o {output file} -cargs : Clustering Arguments JSON : This is a json file containing argument names being mapped to their values. If the custom clusterer doesn't take any arguments (e.g. Infomap). The json will look like the following {} Else, here is an example arguments json for Markov Clustering (MCL): json { \"inflation\": 1.8 } -cfile : Clustering Algorithm Code : Path to your Python wrapper for the clustering algorithm. See this wrapper for example: Infomap . For instructions on writing your own wrapper, see here .","title":"CM++ Usage"},{"location":"cmpp/#cm-usage","text":"","title":"CM++ Usage"},{"location":"cmpp/#basic-usage","text":"python -m hm01.cm -i {input network} -e {existing clustering (optional)} -c {clustering algorithm} -t {connectivity threshold} -n {number of processors (optional)} -o {output file} -i Input Network : The input .tsv edgelist filename. -e Existing Clustering : The existing clustering filename. This is a .tsv with each line being 'node_id cluster_id' format. If this isn't provided, CM++ will run an initial clustering on its own. -c Clustering Algorithm : The clustering paradigm to be used by CM++. Can choose from leiden , leiden_mod , ikc , and external . -t Connectivity Threshold : The connectivity threshold that every cluster must have at least to be considered 'well-connected'. This threshold can be a static integer e.g. 2 or a linear combination of: log10 : $log10(n)$ where $n$ is the number of nodes in the cluster k : the k-core value of the cluster mcd : the minimum degree value e.g. 1log10 , 1log10+1mcd+2k . -n Number of Processors : The number of cores to run CM++ in parallel. This defaults to 4. -o Output File : The output clustering file path. This is a 'node_id cluster_id' .tsv. For example, you can run: python -m hm01.cm -i network.tsv -e clustering.tsv -c leiden -g 0.01 -t 1log10 -n 32 -o output.tsv","title":"Basic Usage"},{"location":"cmpp/#external-clusterers","text":"If you want to use an external clustering algorithm, use the following command format: python -m hm01.cm -i {input network} -e {existing clustering (optional)} -c external -cargs {clustering arguments json} -cfile {clustering algorithm code} -t {connectivity threshold} -n {number of processors (optional)} -o {output file} -cargs : Clustering Arguments JSON : This is a json file containing argument names being mapped to their values. If the custom clusterer doesn't take any arguments (e.g. Infomap). The json will look like the following {} Else, here is an example arguments json for Markov Clustering (MCL): json { \"inflation\": 1.8 } -cfile : Clustering Algorithm Code : Path to your Python wrapper for the clustering algorithm. See this wrapper for example: Infomap . For instructions on writing your own wrapper, see here .","title":"External Clusterers"},{"location":"contributions/","text":"Contributing to the CM++ Pipeline We welcome all helpful contributions to improving CM++ and the CM pipeline. Contributors can help in the following ways: Identifying and/or fixing bugs and typos Writing tutorials and blog posts Creating tests and adding them to the test set Proposing new features How to Contribute Raise issues . Issues can include bugs and errors with installation and usage, or just suggestions for new features. Before raising an issue, please make sure of the following: Double check that your issue isnt already addressed in the documentation on this website. If you are running into problems with code, ensure that the dependencies and your version of the CM++ Pipeline are up to date Check if an issue has already been posted that addresses your concern. Commit and pull request to a forked repository Adding tutorials and blog posts to this wiki Community Guidelines Please make contributions on a forked repository and we will approve pull requests CM++ is majority written in Python. Please ensure you are following the style guidelines Please do not use submodules in your pull requests, package them and include them as a part of requirements.txt Use appropiate version tags. We follow a standard version.feature.patch format for version tags. Make sure you pass all test cases before making a pull request. Using Appropriate Language In commit messages, wiki and blog-posts, and issues, we expect that contributors create a positive environment by: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Focusing on what is best for the community Showing empathy towards other community members We expect that contributors refrain from The use of sexualized language or imagery and unwelcome sexual attention or advances Insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing your own or others\u2019 private information, such as a physical or electronic addresses Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Contributing"},{"location":"contributions/#contributing-to-the-cm-pipeline","text":"We welcome all helpful contributions to improving CM++ and the CM pipeline. Contributors can help in the following ways: Identifying and/or fixing bugs and typos Writing tutorials and blog posts Creating tests and adding them to the test set Proposing new features","title":"Contributing to the CM++ Pipeline"},{"location":"contributions/#how-to-contribute","text":"Raise issues . Issues can include bugs and errors with installation and usage, or just suggestions for new features. Before raising an issue, please make sure of the following: Double check that your issue isnt already addressed in the documentation on this website. If you are running into problems with code, ensure that the dependencies and your version of the CM++ Pipeline are up to date Check if an issue has already been posted that addresses your concern. Commit and pull request to a forked repository Adding tutorials and blog posts to this wiki","title":"How to Contribute"},{"location":"contributions/#community-guidelines","text":"Please make contributions on a forked repository and we will approve pull requests CM++ is majority written in Python. Please ensure you are following the style guidelines Please do not use submodules in your pull requests, package them and include them as a part of requirements.txt Use appropiate version tags. We follow a standard version.feature.patch format for version tags. Make sure you pass all test cases before making a pull request.","title":"Community Guidelines"},{"location":"contributions/#using-appropriate-language","text":"In commit messages, wiki and blog-posts, and issues, we expect that contributors create a positive environment by: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Focusing on what is best for the community Showing empathy towards other community members We expect that contributors refrain from The use of sexualized language or imagery and unwelcome sexual attention or advances Insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing your own or others\u2019 private information, such as a physical or electronic addresses Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Using Appropriate Language"},{"location":"dev_setup/","text":"For Developers Loading a Developer Environment To quickly set up a developer environment for the CM++ Pipeline, simply run the following commands. ( NOTE: Make sure you have Conda installed ) conda env create -f environment.yml conda activate Customizing the Pipeline The CM++ Pipeline also allows for users to add their own pipeline stages and clustering methods. See the following tutorial for adding new stages and algorithms to the pipeline: Pipeline Customization","title":"Developer Setup"},{"location":"dev_setup/#for-developers","text":"","title":"For Developers"},{"location":"dev_setup/#loading-a-developer-environment","text":"To quickly set up a developer environment for the CM++ Pipeline, simply run the following commands. ( NOTE: Make sure you have Conda installed ) conda env create -f environment.yml conda activate","title":"Loading a Developer Environment"},{"location":"dev_setup/#customizing-the-pipeline","text":"The CM++ Pipeline also allows for users to add their own pipeline stages and clustering methods. See the following tutorial for adding new stages and algorithms to the pipeline: Pipeline Customization","title":"Customizing the Pipeline"},{"location":"examples/","text":"Example Commands CM++ python3 -m hm01.cm -i network.tsv -e clustering.tsv -o output.tsv -c leiden -g 0.5 -t 1log10 --nprocs 4 --quiet Runs CM++ on a Leiden with resolution 0.5 clustering with connectivity threshold log10(n) (Every cluster with connectivity over the log of the number of nodes n is considered \"well-connected\") python3 -m hm01.cm -i network.tsv -e clustering.tsv -o output.tsv -c ikc -k 10 -t 1log10 --nprocs 4 --quiet Similar idea but with IKC having hyperparameter k=10 . CM Pipeline Suppose we have a network and a clustering network.tsv clustering.tsv We can then construct the following pipeline.json : { \"title\": \"example\", \"name\": \"example\", \"input_file\": \"network.tsv\", \"output_dir\": \"samples/\", \"algorithm\": \"leiden\", \"params\": [{ \"res\": 0.5, \"i\": 2, \"existing_clustering\": \"clustering.tsv\" }], \"stages\": [ { \"name\": \"connectivity_modifier\", \"memprof\": false, \"threshold\": \"1log10\", \"nprocs\": 1, \"quiet\": true } ] } Then from the root of this repository run: python -m main pipeline.json","title":"Example Commands"},{"location":"examples/#example-commands","text":"","title":"Example Commands"},{"location":"examples/#cm","text":"python3 -m hm01.cm -i network.tsv -e clustering.tsv -o output.tsv -c leiden -g 0.5 -t 1log10 --nprocs 4 --quiet Runs CM++ on a Leiden with resolution 0.5 clustering with connectivity threshold log10(n) (Every cluster with connectivity over the log of the number of nodes n is considered \"well-connected\") python3 -m hm01.cm -i network.tsv -e clustering.tsv -o output.tsv -c ikc -k 10 -t 1log10 --nprocs 4 --quiet Similar idea but with IKC having hyperparameter k=10 .","title":"CM++"},{"location":"examples/#cm-pipeline","text":"Suppose we have a network and a clustering network.tsv clustering.tsv We can then construct the following pipeline.json : { \"title\": \"example\", \"name\": \"example\", \"input_file\": \"network.tsv\", \"output_dir\": \"samples/\", \"algorithm\": \"leiden\", \"params\": [{ \"res\": 0.5, \"i\": 2, \"existing_clustering\": \"clustering.tsv\" }], \"stages\": [ { \"name\": \"connectivity_modifier\", \"memprof\": false, \"threshold\": \"1log10\", \"nprocs\": 1, \"quiet\": true } ] } Then from the root of this repository run: python -m main pipeline.json","title":"CM Pipeline"},{"location":"features/","text":"Features CM Pipeline Features The CM Pipeline is a modular pipeline for community detection that contains the following modules: Graph Cleaning : Removal of parallel and duplicate edges as well as self loops Community Detection : Clusters an input network with one of Leiden, IKC, and InfoMap. Cluster Filtration : A pre-processing stage that allows users to filter out clusters that are trees or have size below a given threshold. Community Statistics Reporting : Generates node and edge count, modularity score, Constant Potts Model score, conductance, and edge-connectivity at multiple stages. Extensibility : Developers can design new stages and wire in new algorithms. Please see the following document for instructions on how to expand the list of supported clustering algorithms as a developer. CM++ CM++ Features CM++ is a module within the CM Pipeline, having the following features: Function : CM++ refines your existing graph clustering by carving them into well-connected clusters with high minimum cut values. Flexibility : Users can accompany their definition of a good community with well-connectedness. CM++ works with any clustering algorithm and presently provides build in support for Leiden, IKC, and Infomap. Dynamic Thresholding : Connectivity thresholds can be constants, or functions of the number of nodes in the cluster, or the minimum node degree of the cluster. Multi-processing : For better performance, users can specify a larger number of cores to process clusters concurrently.","title":"Features"},{"location":"features/#features","text":"","title":"Features"},{"location":"features/#cm-pipeline-features","text":"The CM Pipeline is a modular pipeline for community detection that contains the following modules: Graph Cleaning : Removal of parallel and duplicate edges as well as self loops Community Detection : Clusters an input network with one of Leiden, IKC, and InfoMap. Cluster Filtration : A pre-processing stage that allows users to filter out clusters that are trees or have size below a given threshold. Community Statistics Reporting : Generates node and edge count, modularity score, Constant Potts Model score, conductance, and edge-connectivity at multiple stages. Extensibility : Developers can design new stages and wire in new algorithms. Please see the following document for instructions on how to expand the list of supported clustering algorithms as a developer. CM++","title":"CM Pipeline Features"},{"location":"features/#cm-features","text":"CM++ is a module within the CM Pipeline, having the following features: Function : CM++ refines your existing graph clustering by carving them into well-connected clusters with high minimum cut values. Flexibility : Users can accompany their definition of a good community with well-connectedness. CM++ works with any clustering algorithm and presently provides build in support for Leiden, IKC, and Infomap. Dynamic Thresholding : Connectivity thresholds can be constants, or functions of the number of nodes in the cluster, or the minimum node degree of the cluster. Multi-processing : For better performance, users can specify a larger number of cores to process clusters concurrently.","title":"CM++ Features"},{"location":"installation/","text":"Installation and Setup There are several strategies for installation. First, be sure to confirm that your system meets the requirements for installation. Requirements MacOS or Linux operating system python3.9 or higher cmake 3.2.0 or higher gcc of any version (In our analysis, gcc 9.2.0 was used) Installation via Cloning Clone the cm_pipeline repository Activate the venv which has the necessary packages Run pip install -r requirements.txt && pip install . Make sure everything installed properly by running cd tests && pytest Installation via pip install Simply run pip install git+https://github.com/illinois-or-research-analytics/cm_pipeline . This will install CM++, but to use pipeline functionality, please setup via cloning.","title":"Installation"},{"location":"installation/#installation-and-setup","text":"There are several strategies for installation. First, be sure to confirm that your system meets the requirements for installation.","title":"Installation and Setup"},{"location":"installation/#requirements","text":"MacOS or Linux operating system python3.9 or higher cmake 3.2.0 or higher gcc of any version (In our analysis, gcc 9.2.0 was used)","title":"Requirements"},{"location":"installation/#installation-via-cloning","text":"Clone the cm_pipeline repository Activate the venv which has the necessary packages Run pip install -r requirements.txt && pip install . Make sure everything installed properly by running cd tests && pytest","title":"Installation via Cloning"},{"location":"installation/#installation-via-pip-install","text":"Simply run pip install git+https://github.com/illinois-or-research-analytics/cm_pipeline . This will install CM++, but to use pipeline functionality, please setup via cloning.","title":"Installation via pip install"},{"location":"json_format/","text":"JSON Input Documentation The following document will go over the different parameters for each stage as well as rules and limitations for each stage. To view a valid pipeline with all parameters included, refer to pipeline_template.json . JSON Input Documentation Overall Parameters Algorithmic Parameters Leiden-CPM Leiden-Mod IKC Infomap Your Own Clustering Method Stages Cleanup Clustering Filtering Connectivity Modifier Stats Using an Existing Clustering Examples Overall Parameters The following is a general overview of the overall parameters that don't belong to a single stage but rather the entire pipeline: \"title\": \"cit-new-pp-output\", \"name\": \"cit_patents\", \"input_file\": \"/data3/chackoge/networks/cit_patents_cleaned.tsv\", \"output_dir\": \"samples/\", \"algorithm\": \"leiden\", \"params\": [ { \"res\": 0.5, \"i\": 2 }, { \"res\": 0.1, \"i\": 2 }, { \"res\": 0.01, \"i\": 2 } ] \"stages\": [\"...\"] All of the following parameter values are required: title : The name of the run, can be any string that is descriptive of the pipeline name : The name of the network, again can be any string that is descriptive of the network input_file : The filename of the input network. Is an edgelist .tsv. output_dir : The directory to store pipeline outputs. algorithm : The name of the algorithm. Can choose from \"leiden\" , \"leiden_mod\" , and \"ikc\" . stages : An array of stage objects. params : A list of dictionaries mapping algorithm parameters to their values. NOTE Paths must be relative to the json file, or absolute. Algorithmic Parameters The params field contains dictionaries mapping parameter values to their values. These field names will vary based on the algorithm being used. Leiden-CPM As shown above, Leiden-CPM takes resolution and iterations parameters. Designated in the json as \"res\" and \"i\" fields. { \"res\": 0.5, \"i\": 2 } Leiden-Mod Leiden-Mod doesn't need to use a resolution parameter since it optimizes modularity and not CPM. Therefore, only an iterations parameter needs to be passed. { \"i\": 2 } IKC IKC only needs a k-core value passed as a parameter, designated as \"k\" . { \"k\": 10 } Infomap Infomap doesn't take ay parameters, so its dictionary is always empty. However, to fit the style constraints of the pipeline, any algorithm that doesnt take any parameters should use an empty dictionary as follows. {} Since you can't have multiple runs of the same parameter set, the overall \"params\" field in the json will look like this: \"params\": [{}] Your Own Clustering Method Refer to the customization documentation for more details on how to create your own clustering. You will be able to assign parameter names for your own clustering using this pipeline. Supposing you create a pipeline with two parameters \"a\" and \"b\" with integer values, you will be able to designate them in the pipeline file. { \"a\": 1, \"b\": 2 } Stages Cleanup This stage removes any self loops (i.e. edges $(u, u)$ ) and parallel edges (i.e. duplicate edges $(u, v)$ with more than one occurrence in the edge list). This stage does not take any extra parameters and has the following syntax. Add the following object in the stages array: { \"name\": \"cleanup\" } Limitations : This stage cannot come after a stage that outputs a clustering (ex. filtering, connectivity_modifier). Clustering This stage uses the clustering algorithm specified in the overall parameters to cluster a cleaned network. If resolutions and/or iterations are arrays, multiple clusterings are outputted. To add this stage, add the following to the stages array. Modify the parameters as needed: { \"name\": \"clustering\", \"parallel_limit\": 2 } Optional Parameter : parallel_limit : The number of clustering jobs that can be run in parallel. This is useful if resolutions or iterations are arrays. In the example above, clustering jobs will be run in pairs of twos. If the limit is 1, clustering jobs will be run sequentially. If no limit is specified, all clustering jobs will be run in parallel. Limitations : This stage cannot come after a stage that outputs a clustering. Filtering This stage takes a clustering and filters it according to a script, or series of scripts. To add this stage, add the following to the stages array. Modify the scripts as needed.: { \"name\": \"filtering\", \"scripts\": [ \"./scripts/subset_graph_nonetworkit_treestar.R\", \"./scripts/make_cm_ready.R\" ] } Required Paramters : scripts : This is an array of script file names. If you only want to run one script, the array will have one element with the script name. The following are scripts in the repository that can be used for filtration: \"subset_graph_nonetworkit_treestar.R\" \"make_cm_ready.R\" \"post_cm_filter.R\" Limitations : This stage must come after a stage that outputs a clustering. Connectivity Modifier This is the stage that applies CM++ to a clustering to ensure connectivity requirements in clusters. To add the stage, simply add the following to the stages array. Change the parameters as needed. If the parameters are optional, they can be deleted from this template: { \"name\": \"connectivity_modifier\", \"memprof\": true, \"threshold\": \"1log10\", \"nprocs\": 32, \"quiet\": true, } Required Parameter : threshold : The connectivity threshold. This is a string representing a linear combination between log10 , mcd (minimum core degree), k (IKC only), and a constant. The following are valid thresholds 1log10 1log10+4mcd+1k+4 mcd Optional Parameters : memprof : Profile the memory usage per process over time as CM++ is running. If this is ommitted or set to false, memory profiling will not run. nprocs : Number of cores to run CM++. If omitted, this defaults to 4. quiet : Silence output to terminal. If omitted, this defaults to false. Limitations : This must come after a stage that outputs a clustering. NOTE: If using IKC, it's recommended to run no more than 4 processors on CM Stats This stage reports statistics of a clustering that was outputted by a stage preceding it. For more information on the statistics reporting ans its outputs. The code for the stage is the following: { \"name\": \"stats\", \"parallel_limit\": 2, \"universal_before\": false, \"summarize\": false } Optional Parameters : parallel_limit : This is the same as for the clustering stage universal_before : Output extra details on which clusters were split by CM. If ommitted, this defaults to false . summarize : Output more detailed summary statistics for the clustering overall. Limitations : This stage must come after a stage that outputs a clustering. If universal_before is a field, CM must appear sometime before this stats stage ( NOTE : Not necessarily one stage before, can be at any point preceding this stats stage). Using an Existing Clustering { \"title\": \"cit-new-pp-output-leiden-skipstage\", \"name\": \"cit_patents\", \"input_file\": \"/data3/chackoge/networks/cit_patents_cleaned.tsv\", \"output_dir\": \"samples/\", \"algorithm\": \"leiden\", \"params\": [ { \"res\": 0.5, \"i\": 2, \"existing_clustering\": \"samples/cit-new-pp-output-leiden_mod-20230614-23:55:59/res-0.5-i2/S2_cit_patents_leiden.0.5_i2_clustering.tsv\" }, { \"res\": 0.1, \"i\": 2, \"existing_clustering\": \"samples/cit-new-pp-output-leiden_mod-20230614-23:55:59/res-0.1-i2/S2_cit_patents_leiden.0.1_i2_clustering.tsv\" } ], \"stages\": [\"...\"] } To use an existing clustering, add a value \"existing_clustering\" per parameter entry in your json header. This is applicable for any clustering method. Examples View the following folder to check out examples: examples/","title":"JSON Format Tutorial"},{"location":"json_format/#json-input-documentation","text":"The following document will go over the different parameters for each stage as well as rules and limitations for each stage. To view a valid pipeline with all parameters included, refer to pipeline_template.json . JSON Input Documentation Overall Parameters Algorithmic Parameters Leiden-CPM Leiden-Mod IKC Infomap Your Own Clustering Method Stages Cleanup Clustering Filtering Connectivity Modifier Stats Using an Existing Clustering Examples","title":"JSON Input Documentation"},{"location":"json_format/#overall-parameters","text":"The following is a general overview of the overall parameters that don't belong to a single stage but rather the entire pipeline: \"title\": \"cit-new-pp-output\", \"name\": \"cit_patents\", \"input_file\": \"/data3/chackoge/networks/cit_patents_cleaned.tsv\", \"output_dir\": \"samples/\", \"algorithm\": \"leiden\", \"params\": [ { \"res\": 0.5, \"i\": 2 }, { \"res\": 0.1, \"i\": 2 }, { \"res\": 0.01, \"i\": 2 } ] \"stages\": [\"...\"] All of the following parameter values are required: title : The name of the run, can be any string that is descriptive of the pipeline name : The name of the network, again can be any string that is descriptive of the network input_file : The filename of the input network. Is an edgelist .tsv. output_dir : The directory to store pipeline outputs. algorithm : The name of the algorithm. Can choose from \"leiden\" , \"leiden_mod\" , and \"ikc\" . stages : An array of stage objects. params : A list of dictionaries mapping algorithm parameters to their values. NOTE Paths must be relative to the json file, or absolute.","title":"Overall Parameters"},{"location":"json_format/#algorithmic-parameters","text":"The params field contains dictionaries mapping parameter values to their values. These field names will vary based on the algorithm being used.","title":"Algorithmic Parameters"},{"location":"json_format/#leiden-cpm","text":"As shown above, Leiden-CPM takes resolution and iterations parameters. Designated in the json as \"res\" and \"i\" fields. { \"res\": 0.5, \"i\": 2 }","title":"Leiden-CPM"},{"location":"json_format/#leiden-mod","text":"Leiden-Mod doesn't need to use a resolution parameter since it optimizes modularity and not CPM. Therefore, only an iterations parameter needs to be passed. { \"i\": 2 }","title":"Leiden-Mod"},{"location":"json_format/#ikc","text":"IKC only needs a k-core value passed as a parameter, designated as \"k\" . { \"k\": 10 }","title":"IKC"},{"location":"json_format/#infomap","text":"Infomap doesn't take ay parameters, so its dictionary is always empty. However, to fit the style constraints of the pipeline, any algorithm that doesnt take any parameters should use an empty dictionary as follows. {} Since you can't have multiple runs of the same parameter set, the overall \"params\" field in the json will look like this: \"params\": [{}]","title":"Infomap"},{"location":"json_format/#your-own-clustering-method","text":"Refer to the customization documentation for more details on how to create your own clustering. You will be able to assign parameter names for your own clustering using this pipeline. Supposing you create a pipeline with two parameters \"a\" and \"b\" with integer values, you will be able to designate them in the pipeline file. { \"a\": 1, \"b\": 2 }","title":"Your Own Clustering Method"},{"location":"json_format/#stages","text":"","title":"Stages"},{"location":"json_format/#cleanup","text":"This stage removes any self loops (i.e. edges $(u, u)$ ) and parallel edges (i.e. duplicate edges $(u, v)$ with more than one occurrence in the edge list). This stage does not take any extra parameters and has the following syntax. Add the following object in the stages array: { \"name\": \"cleanup\" } Limitations : This stage cannot come after a stage that outputs a clustering (ex. filtering, connectivity_modifier).","title":"Cleanup"},{"location":"json_format/#clustering","text":"This stage uses the clustering algorithm specified in the overall parameters to cluster a cleaned network. If resolutions and/or iterations are arrays, multiple clusterings are outputted. To add this stage, add the following to the stages array. Modify the parameters as needed: { \"name\": \"clustering\", \"parallel_limit\": 2 } Optional Parameter : parallel_limit : The number of clustering jobs that can be run in parallel. This is useful if resolutions or iterations are arrays. In the example above, clustering jobs will be run in pairs of twos. If the limit is 1, clustering jobs will be run sequentially. If no limit is specified, all clustering jobs will be run in parallel. Limitations : This stage cannot come after a stage that outputs a clustering.","title":"Clustering"},{"location":"json_format/#filtering","text":"This stage takes a clustering and filters it according to a script, or series of scripts. To add this stage, add the following to the stages array. Modify the scripts as needed.: { \"name\": \"filtering\", \"scripts\": [ \"./scripts/subset_graph_nonetworkit_treestar.R\", \"./scripts/make_cm_ready.R\" ] } Required Paramters : scripts : This is an array of script file names. If you only want to run one script, the array will have one element with the script name. The following are scripts in the repository that can be used for filtration: \"subset_graph_nonetworkit_treestar.R\" \"make_cm_ready.R\" \"post_cm_filter.R\" Limitations : This stage must come after a stage that outputs a clustering.","title":"Filtering"},{"location":"json_format/#connectivity-modifier","text":"This is the stage that applies CM++ to a clustering to ensure connectivity requirements in clusters. To add the stage, simply add the following to the stages array. Change the parameters as needed. If the parameters are optional, they can be deleted from this template: { \"name\": \"connectivity_modifier\", \"memprof\": true, \"threshold\": \"1log10\", \"nprocs\": 32, \"quiet\": true, } Required Parameter : threshold : The connectivity threshold. This is a string representing a linear combination between log10 , mcd (minimum core degree), k (IKC only), and a constant. The following are valid thresholds 1log10 1log10+4mcd+1k+4 mcd Optional Parameters : memprof : Profile the memory usage per process over time as CM++ is running. If this is ommitted or set to false, memory profiling will not run. nprocs : Number of cores to run CM++. If omitted, this defaults to 4. quiet : Silence output to terminal. If omitted, this defaults to false. Limitations : This must come after a stage that outputs a clustering. NOTE: If using IKC, it's recommended to run no more than 4 processors on CM","title":"Connectivity Modifier"},{"location":"json_format/#stats","text":"This stage reports statistics of a clustering that was outputted by a stage preceding it. For more information on the statistics reporting ans its outputs. The code for the stage is the following: { \"name\": \"stats\", \"parallel_limit\": 2, \"universal_before\": false, \"summarize\": false } Optional Parameters : parallel_limit : This is the same as for the clustering stage universal_before : Output extra details on which clusters were split by CM. If ommitted, this defaults to false . summarize : Output more detailed summary statistics for the clustering overall. Limitations : This stage must come after a stage that outputs a clustering. If universal_before is a field, CM must appear sometime before this stats stage ( NOTE : Not necessarily one stage before, can be at any point preceding this stats stage).","title":"Stats"},{"location":"json_format/#using-an-existing-clustering","text":"{ \"title\": \"cit-new-pp-output-leiden-skipstage\", \"name\": \"cit_patents\", \"input_file\": \"/data3/chackoge/networks/cit_patents_cleaned.tsv\", \"output_dir\": \"samples/\", \"algorithm\": \"leiden\", \"params\": [ { \"res\": 0.5, \"i\": 2, \"existing_clustering\": \"samples/cit-new-pp-output-leiden_mod-20230614-23:55:59/res-0.5-i2/S2_cit_patents_leiden.0.5_i2_clustering.tsv\" }, { \"res\": 0.1, \"i\": 2, \"existing_clustering\": \"samples/cit-new-pp-output-leiden_mod-20230614-23:55:59/res-0.1-i2/S2_cit_patents_leiden.0.1_i2_clustering.tsv\" } ], \"stages\": [\"...\"] } To use an existing clustering, add a value \"existing_clustering\" per parameter entry in your json header. This is applicable for any clustering method.","title":"Using an Existing Clustering"},{"location":"json_format/#examples","text":"View the following folder to check out examples: examples/","title":"Examples"},{"location":"pipeline_customization/","text":"Pipeline Modification Documentation The CM Pipeline allows for macros and modifications that developers can insert. You will be able to modify the pipeline in the two following ways: Use your own clustering method Build your own pipeline stages Using your own clustering method First, to use your own clustering method, follow both of these procedures: I. Inserting your clustering method into CM++ From root, navigate to the hm01/clusterers/external_clusterers/ directory Create a clusterer object that calls your clustering method. Here is a template: from dataclasses import dataclass from typing import List, Iterator, Dict, Union from hm01.clusterers.abstract_clusterer import AbstractClusterer from hm01.graph import Graph, IntangibleSubgraph, RealizedSubgraph @dataclass class TemplateClusterer(AbstractClusterer): def __init__(args): # Create a clusterer object. Args is the arguments of the clusterer # Ex. Leiden-CPM would have resolution as an arg pass def cluster(self, graph: Union[Graph, RealizedSubgraph]) -> Iterator[IntangibleSubgraph]: # Return an iterator of intangible subgraphs representing your resultant clusters pass def getclusterer(args): # Construct the clusterer object from the args return TemplateClusterer(args) # Ex. Leiden-CPM's clusterer would be getclusterer(resolution) Then when you call CM++, you will create a JSON file mapping arguments to their values. Here is a template/example { \"arg\": \"val\", \"resolution\": 0.5 } Then, when you call CM++, you can do the following: python -m hm01.cm -i network.tsv -e clustering.tsv -cfile hm01/clusterers/MyWrapper.py -cargs MyArguments.json -t 1log10 II. Inserting your clustering method into the pipeline Navigate to source/clusterers/ Create a python object file to wrap your clustering method. Here is a template: from source.clustering import Clustering class LeidenModClustering(Clustering): def __init__( self, data, input_file, network_name, resolutions, iterations, algorithm, existing_clustering, working_dir, index): super().__init__( data, input_file, network_name, resolutions, iterations, algorithm, existing_clustering, working_dir, index) def initialize_clustering(self): self.output_file = [ # process list of parameter sets into output file names # For example, if Leiden CPM has parameter set # [{ # \"res\": 0.5, # \"i\": 2 # }, { # \"res\": 0.1, # \"i\": 1 # }] # You will need two output files. One for res-0.5-i2 and one for res-0.1-i1 for param in self.params ] def get_stage_commands(self, project_root, prev_file): # Write code that returns an array of shell commands that run your clustering method. # The array of commands needs to be per, and in the same order, as your params set # Refer to self.params pass Navigate to source/typedict.py . In the cluster_classes dictionary. Add a mapping from your clustering algorithm name to the object that you had created. Remember to import your clusterer! E. 'mcl': MCL . To run the pipeline with your new clusterer. Do the following: Create a json file (refer to pipeline.json for an example) containing the parameter set that you would like to run for your method. This set will have multiple sets of parameters if you want to have multiple runs of your pipeline. If your clusterer doesnt take any parameters, your \"params\" field will look like: \"params\": [{}] In the case that CM++ is in your pipeline, make sure your stage has \"cfile\" in the parameters. Note that you do not need a \"cargs\" parameter as the pipeline will automatically create an args json. Run python -m main pipeline.json from root. Example: Infomap First, I created the infomap wrapper as shown in this file . The cluster method simply uses python's Infomap library, and converts the outputs into hm01 IntangibleSubgraph objects. The get_clusterer method doesn't take any arguments since InfoMap doesn't require any parameters Second, in this clusterer object , I created a clusterer object for the pipeline. InfoMap is quite simple, it doesn't take any parameters and it doesn't have any extra requirements, so the __init__ method doesn't need any more than it has. The initialize_clustering method simply sets its output file name. You want output in the relevant directory. For infomap, that was f{self.working_dir}/infomap/ . For your method, you should refer to the self.get_folder_name(param) method, where param is the current parameter dictionary. The get_stage_commands method converts the stage object data into a runnable shell command by the pipeline. I have made a run_infomap script that the CM pipeline can call. In the typedict file , I have added keys for infomap Creating your own pipeline stage Navigate to source/ Create an empty stage object. Start with this template. Replace names according to your preferences: from source.stage import Stage class MyStage(Stage): def __init__( self, data, input_file, network_name, resolutions, iterations, algorithm, existing_clustering, working_dir, index ): super().__init__( data, input_file, network_name, resolutions, iterations, algorithm, existing_clustering, working_dir, index) def initialize(self, data): # This method sets required parameters of your stage # The data argument is the stage data in the json (dict) self.chainable = # Can the outputs of this stage be used as an input for the next? self.outputs_clustering = # Does this stage output a clustering or something else? self.output_file = # What filename does this stage output? def get_stage_commands(self, project_root, prev_file): # Return an array of commands that the pipeline will execute when it reaches this stage Navigate to source/typedict.py In stage_classes , modify the disctionary to map a string representing your stage, to the object you created. Make sure to import your code! Now, when writing your pipeline.json , simply add your stage in the \"stages\" array. Use the name specified in the previous step, and the arguments processed in your code. Mincut Filter TODO: This should be tested, and documented here The Stage and Clusterer Objects Extensions of AbstractClusterer To view source code for the abstract class, see here . Objects extending the AbstractClusterer object must have the following: Object variables containing the clusterer parameters: @dataclass class IkcClusterer(AbstractClusterer): k: int A cluster method that runs the clustering algorithm and returns clusters in the form of IntangibleSubgraph objects in hm01. This is really just a set of vertices. This method can also call other class helper methods Your file containing the object extending the AbstractClusterer must contain a get_clusterer method taking in arguments for the clusterer, and returning the clusterer object. This is so that CM can generalize to use your clustering method Extensions of Stage To view the abstract class, click here . Any extension of Stage must contain the following: The __init__ can simply super the abstract class. An initialize(self, data) method to set the following: The data parameter is a dictionary representing the stage object in the json. self.outputs_clustering : A boolean on whether your stage outputs a cluatering or something else For example cleanup and stats outputs a graph and statistics respectively, both of which are not clusterings. self.chainable : A boolean on whether you stage's outputs can be used by the next stage For example, if your stage outputs an aggregated graph that can be reclustered, it is chainable self.output_file If your stage outputs one file, this is a string If your stage outputs a file per parameter set, this is an array following the same order as the params specified in the json. Output files should be stored in the appropriate directory. Use self.get_folder_name(param) to get the folder name for the parameter dictionary used. This means that the correct folder for a param set param would be in f'{self.working_dir}/{self.get_folder_name(param)}/ Any parameters that are specific to your clusterer can be assigned here E.g. self.scripts for the filtration stage A get_stage_commands(self, project_root, prev_file) . The project_root is the root folder for this repository The prev_file is the filename (as a string or array of strings per parameter set). This command should return an array of commands to execute when this stage is reached. These command must address all the parameter sets, and return files per each parameter set. Extensions of Clustering Clustering is already an extension of Stage. To view the parent object, see the code here . Any extension of the clustering object should have: __init__ can simply super the clustering object initialize_clustering(self) . Set the output file when this clusterer is run. This is similar to setting the stage output file. get_stage_commands(self, project_root, previous file) . This returns a set of commands when your clustering method is run. You should have an executable for your clustering that is runnable via shell. If it is a python module (like infomap or Leiden), please make a runnable script (like this one) . If you want to submit your changes, keep your scripts in the scripts/ folder. Submitting your Changes To make your new stages and clustering methods a part of the official repo: Create a fork of this repository Insert your new clustering methods and stages Create a pull request and we will review and approve it","title":"Pipeline Modification Documentation"},{"location":"pipeline_customization/#pipeline-modification-documentation","text":"The CM Pipeline allows for macros and modifications that developers can insert. You will be able to modify the pipeline in the two following ways: Use your own clustering method Build your own pipeline stages","title":"Pipeline Modification Documentation"},{"location":"pipeline_customization/#using-your-own-clustering-method","text":"First, to use your own clustering method, follow both of these procedures:","title":"Using your own clustering method"},{"location":"pipeline_customization/#i-inserting-your-clustering-method-into-cm","text":"From root, navigate to the hm01/clusterers/external_clusterers/ directory Create a clusterer object that calls your clustering method. Here is a template: from dataclasses import dataclass from typing import List, Iterator, Dict, Union from hm01.clusterers.abstract_clusterer import AbstractClusterer from hm01.graph import Graph, IntangibleSubgraph, RealizedSubgraph @dataclass class TemplateClusterer(AbstractClusterer): def __init__(args): # Create a clusterer object. Args is the arguments of the clusterer # Ex. Leiden-CPM would have resolution as an arg pass def cluster(self, graph: Union[Graph, RealizedSubgraph]) -> Iterator[IntangibleSubgraph]: # Return an iterator of intangible subgraphs representing your resultant clusters pass def getclusterer(args): # Construct the clusterer object from the args return TemplateClusterer(args) # Ex. Leiden-CPM's clusterer would be getclusterer(resolution) Then when you call CM++, you will create a JSON file mapping arguments to their values. Here is a template/example { \"arg\": \"val\", \"resolution\": 0.5 } Then, when you call CM++, you can do the following: python -m hm01.cm -i network.tsv -e clustering.tsv -cfile hm01/clusterers/MyWrapper.py -cargs MyArguments.json -t 1log10","title":"I. Inserting your clustering method into CM++"},{"location":"pipeline_customization/#ii-inserting-your-clustering-method-into-the-pipeline","text":"Navigate to source/clusterers/ Create a python object file to wrap your clustering method. Here is a template: from source.clustering import Clustering class LeidenModClustering(Clustering): def __init__( self, data, input_file, network_name, resolutions, iterations, algorithm, existing_clustering, working_dir, index): super().__init__( data, input_file, network_name, resolutions, iterations, algorithm, existing_clustering, working_dir, index) def initialize_clustering(self): self.output_file = [ # process list of parameter sets into output file names # For example, if Leiden CPM has parameter set # [{ # \"res\": 0.5, # \"i\": 2 # }, { # \"res\": 0.1, # \"i\": 1 # }] # You will need two output files. One for res-0.5-i2 and one for res-0.1-i1 for param in self.params ] def get_stage_commands(self, project_root, prev_file): # Write code that returns an array of shell commands that run your clustering method. # The array of commands needs to be per, and in the same order, as your params set # Refer to self.params pass Navigate to source/typedict.py . In the cluster_classes dictionary. Add a mapping from your clustering algorithm name to the object that you had created. Remember to import your clusterer! E. 'mcl': MCL . To run the pipeline with your new clusterer. Do the following: Create a json file (refer to pipeline.json for an example) containing the parameter set that you would like to run for your method. This set will have multiple sets of parameters if you want to have multiple runs of your pipeline. If your clusterer doesnt take any parameters, your \"params\" field will look like: \"params\": [{}] In the case that CM++ is in your pipeline, make sure your stage has \"cfile\" in the parameters. Note that you do not need a \"cargs\" parameter as the pipeline will automatically create an args json. Run python -m main pipeline.json from root.","title":"II. Inserting your clustering method into the pipeline"},{"location":"pipeline_customization/#example-infomap","text":"First, I created the infomap wrapper as shown in this file . The cluster method simply uses python's Infomap library, and converts the outputs into hm01 IntangibleSubgraph objects. The get_clusterer method doesn't take any arguments since InfoMap doesn't require any parameters Second, in this clusterer object , I created a clusterer object for the pipeline. InfoMap is quite simple, it doesn't take any parameters and it doesn't have any extra requirements, so the __init__ method doesn't need any more than it has. The initialize_clustering method simply sets its output file name. You want output in the relevant directory. For infomap, that was f{self.working_dir}/infomap/ . For your method, you should refer to the self.get_folder_name(param) method, where param is the current parameter dictionary. The get_stage_commands method converts the stage object data into a runnable shell command by the pipeline. I have made a run_infomap script that the CM pipeline can call. In the typedict file , I have added keys for infomap","title":"Example: Infomap"},{"location":"pipeline_customization/#creating-your-own-pipeline-stage","text":"Navigate to source/ Create an empty stage object. Start with this template. Replace names according to your preferences: from source.stage import Stage class MyStage(Stage): def __init__( self, data, input_file, network_name, resolutions, iterations, algorithm, existing_clustering, working_dir, index ): super().__init__( data, input_file, network_name, resolutions, iterations, algorithm, existing_clustering, working_dir, index) def initialize(self, data): # This method sets required parameters of your stage # The data argument is the stage data in the json (dict) self.chainable = # Can the outputs of this stage be used as an input for the next? self.outputs_clustering = # Does this stage output a clustering or something else? self.output_file = # What filename does this stage output? def get_stage_commands(self, project_root, prev_file): # Return an array of commands that the pipeline will execute when it reaches this stage Navigate to source/typedict.py In stage_classes , modify the disctionary to map a string representing your stage, to the object you created. Make sure to import your code! Now, when writing your pipeline.json , simply add your stage in the \"stages\" array. Use the name specified in the previous step, and the arguments processed in your code.","title":"Creating your own pipeline stage"},{"location":"pipeline_customization/#mincut-filter","text":"TODO: This should be tested, and documented here","title":"Mincut Filter"},{"location":"pipeline_customization/#the-stage-and-clusterer-objects","text":"","title":"The Stage and Clusterer Objects"},{"location":"pipeline_customization/#extensions-of-abstractclusterer","text":"To view source code for the abstract class, see here . Objects extending the AbstractClusterer object must have the following: Object variables containing the clusterer parameters: @dataclass class IkcClusterer(AbstractClusterer): k: int A cluster method that runs the clustering algorithm and returns clusters in the form of IntangibleSubgraph objects in hm01. This is really just a set of vertices. This method can also call other class helper methods Your file containing the object extending the AbstractClusterer must contain a get_clusterer method taking in arguments for the clusterer, and returning the clusterer object. This is so that CM can generalize to use your clustering method","title":"Extensions of AbstractClusterer"},{"location":"pipeline_customization/#extensions-of-stage","text":"To view the abstract class, click here . Any extension of Stage must contain the following: The __init__ can simply super the abstract class. An initialize(self, data) method to set the following: The data parameter is a dictionary representing the stage object in the json. self.outputs_clustering : A boolean on whether your stage outputs a cluatering or something else For example cleanup and stats outputs a graph and statistics respectively, both of which are not clusterings. self.chainable : A boolean on whether you stage's outputs can be used by the next stage For example, if your stage outputs an aggregated graph that can be reclustered, it is chainable self.output_file If your stage outputs one file, this is a string If your stage outputs a file per parameter set, this is an array following the same order as the params specified in the json. Output files should be stored in the appropriate directory. Use self.get_folder_name(param) to get the folder name for the parameter dictionary used. This means that the correct folder for a param set param would be in f'{self.working_dir}/{self.get_folder_name(param)}/ Any parameters that are specific to your clusterer can be assigned here E.g. self.scripts for the filtration stage A get_stage_commands(self, project_root, prev_file) . The project_root is the root folder for this repository The prev_file is the filename (as a string or array of strings per parameter set). This command should return an array of commands to execute when this stage is reached. These command must address all the parameter sets, and return files per each parameter set.","title":"Extensions of Stage"},{"location":"pipeline_customization/#extensions-of-clustering","text":"Clustering is already an extension of Stage. To view the parent object, see the code here . Any extension of the clustering object should have: __init__ can simply super the clustering object initialize_clustering(self) . Set the output file when this clusterer is run. This is similar to setting the stage output file. get_stage_commands(self, project_root, previous file) . This returns a set of commands when your clustering method is run. You should have an executable for your clustering that is runnable via shell. If it is a python module (like infomap or Leiden), please make a runnable script (like this one) . If you want to submit your changes, keep your scripts in the scripts/ folder.","title":"Extensions of Clustering"},{"location":"pipeline_customization/#submitting-your-changes","text":"To make your new stages and clustering methods a part of the official repo: Create a fork of this repository Insert your new clustering methods and stages Create a pull request and we will review and approve it","title":"Submitting your Changes"},{"location":"pipeline_usage/","text":"Pipeline Usage Edit the fields of the pipeline.json file to reflect your inputs and requirements. Run python -m main pipeline.json JSON Input Documentation To create a JSON pipeline that fits your needs, see the following tutorial Output Files The commands executed during the workflow are captured in {output_dir}/{run_name}-{timestamp}/commands.sh . This is the shell script generated by the pipeline that is run to generate outputs. The output files generated during the workflow are stored in the folder {output_dir}/{run_name}-{timestamp}/ The descriptive analysis files can be found in the folder {output_dir}/{run_name}-{timestamp}/analysis with the *.csv file for each of the resolution values.","title":"CM Pipeline Usage"},{"location":"pipeline_usage/#pipeline-usage","text":"Edit the fields of the pipeline.json file to reflect your inputs and requirements. Run python -m main pipeline.json","title":"Pipeline Usage"},{"location":"pipeline_usage/#json-input-documentation","text":"To create a JSON pipeline that fits your needs, see the following tutorial","title":"JSON Input Documentation"},{"location":"pipeline_usage/#output-files","text":"The commands executed during the workflow are captured in {output_dir}/{run_name}-{timestamp}/commands.sh . This is the shell script generated by the pipeline that is run to generate outputs. The output files generated during the workflow are stored in the folder {output_dir}/{run_name}-{timestamp}/ The descriptive analysis files can be found in the folder {output_dir}/{run_name}-{timestamp}/analysis with the *.csv file for each of the resolution values.","title":"Output Files"},{"location":"quickstart/","text":"Quick Start Guide Suppose we have a network that we want to cluster and refine using the pipeline. For this tutorial, I will use the following network and clustering. network.tsv clustering.tsv First, let's visualize the graph and clustering","title":"Quick Start Guide"},{"location":"quickstart/#quick-start-guide","text":"Suppose we have a network that we want to cluster and refine using the pipeline. For this tutorial, I will use the following network and clustering. network.tsv clustering.tsv First, let's visualize the graph and clustering","title":"Quick Start Guide"}]}